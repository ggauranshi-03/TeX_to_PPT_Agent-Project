{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WIikUQ63vwTk",
        "outputId": "24d965bc-c624-42bd-c3e6-e74f4a0c50fe"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y texlive-latex-recommended texlive-fonts-recommended texlive-latex-extra texlive-fonts-extra texlive-science texlive-xetex dvipng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cpk4BRRzvy8o",
        "outputId": "5b4f6d4e-9ece-4426-c178-6df221f8e767"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y texlive-science texlive-fonts-extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iZAIVFraxDnJ",
        "outputId": "46c7dbe3-cfbd-4af2-b0ad-c3c7761557fe"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisP49iCDRAI"
      },
      "source": [
        "**Single Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hyb1WJqcv4e3",
        "outputId": "9fcc70be-52cc-4781-f83d-10ac882d9566"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import subprocess\n",
        "import shutil\n",
        "from typing import TypedDict, List, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_NAME = \"sonar\"\n",
        "\n",
        "# --- 1. The Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    tar_path: str\n",
        "    work_dir: str\n",
        "    tex_content: str\n",
        "    beamer_code: str\n",
        "    pdf_path: str\n",
        "    available_images: List[str]\n",
        "    feedback: Optional[str]\n",
        "    error_log: Optional[str]\n",
        "    iterations: int\n",
        "\n",
        "# --- 2. The Nodes ---\n",
        "\n",
        "def extract_node(state: AgentState):\n",
        "    path = state['tar_path']\n",
        "    # Use absolute paths to avoid confusion\n",
        "    extract_to = os.path.abspath(path.replace(\".tar.gz\", \"_work\").replace(\".tar\", \"_work\"))\n",
        "\n",
        "    if os.path.exists(extract_to):\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    with tarfile.open(path, \"r:*\") as tar:\n",
        "        tar.extractall(path=extract_to, filter='fully_trusted')\n",
        "\n",
        "    main_file = None\n",
        "    image_files = []\n",
        "\n",
        "    for root, _, fs in os.walk(extract_to):\n",
        "        for f in fs:\n",
        "            full_path = os.path.join(root, f)\n",
        "            # 1. Collect images\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.pdf')):\n",
        "                image_files.append(f)\n",
        "\n",
        "            # 2. Find the main TeX file (contains \\begin{document})\n",
        "            if f.endswith(\".tex\"):\n",
        "                with open(full_path, 'r', errors='ignore') as tex:\n",
        "                    if \"\\\\begin{document}\" in tex.read():\n",
        "                        main_file = full_path\n",
        "\n",
        "    if not main_file:\n",
        "        # Fallback: if no main file found, just take the largest .tex file\n",
        "        tex_files = [os.path.join(root, f) for root, _, fs in os.walk(extract_to) for f in fs if f.endswith('.tex')]\n",
        "        if tex_files:\n",
        "            main_file = max(tex_files, key=os.path.getsize)\n",
        "        else:\n",
        "            raise ValueError(\"Could not find a .tex file in the archive.\")\n",
        "\n",
        "    with open(main_file, 'r', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    return {\n",
        "        \"work_dir\": extract_to,\n",
        "        \"tex_content\": content,\n",
        "        \"available_images\": image_files,\n",
        "        \"iterations\": 0\n",
        "    }\n",
        "\n",
        "def draft_slides_node(state: AgentState):\n",
        "    llm = ChatOpenAI(\n",
        "        api_key=\"\",\n",
        "        base_url=\"https://api.perplexity.ai\",\n",
        "        model=MODEL_NAME\n",
        "    )\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a LaTeX Beamer expert. Create a professional 16:9 presentation. \"\n",
        "        \"IMPORTANT: Only use standard packages like graphicx, amsmath, and amssymb. \"\n",
        "        \"Do NOT use stmaryrd or custom .sty files unless explicitly asked. \"\n",
        "        \"Return ONLY the raw LaTeX code beginning with \\\\documentclass.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"Original Paper Content: {state['tex_content'][:5000]}\\n\"\n",
        "    user_prompt += f\"Available Image Files: {state['available_images']}\\n\"\n",
        "\n",
        "    if state.get('feedback'):\n",
        "        user_prompt += f\"\\nRefine the previous version. User wants to improve: {state['feedback']}\\nPrevious Code: {state['beamer_code']}\"\n",
        "\n",
        "    if state.get('error_log'):\n",
        "        user_prompt += f\"\\n\\nFIX THIS ERROR: {state['error_log']}\"\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "\n",
        "    clean_code = response.content.replace(\"```latex\", \"\").replace(\"```\", \"\").strip()\n",
        "    return {\"beamer_code\": clean_code, \"iterations\": state['iterations'] + 1}\n",
        "\n",
        "def compile_node(state: AgentState):\n",
        "    tex_file = os.path.join(state['work_dir'], \"presentation.tex\")\n",
        "    with open(tex_file, \"w\") as f:\n",
        "        f.write(state['beamer_code'])\n",
        "\n",
        "    # Capture raw output to avoid UnicodeDecodeError\n",
        "    result = subprocess.run(\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"presentation.tex\"],\n",
        "        cwd=state['work_dir'],\n",
        "        capture_output=True\n",
        "    )\n",
        "\n",
        "    # Safely decode the logs\n",
        "    stdout_log = result.stdout.decode('utf-8', errors='replace')\n",
        "\n",
        "    pdf_out = os.path.join(state['work_dir'], \"presentation.pdf\")\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        return {\"error_log\": stdout_log[-800:], \"pdf_path\": \"\"}\n",
        "\n",
        "    # Double check if PDF actually exists even if return code was 0\n",
        "    if os.path.exists(pdf_out):\n",
        "        return {\"pdf_path\": pdf_out, \"error_log\": None}\n",
        "    else:\n",
        "        return {\"error_log\": stdout_log[-800:], \"pdf_path\": \"\"}\n",
        "\n",
        "# --- 3. Build Graph Logic ---\n",
        "\n",
        "def route_after_compile(state: AgentState):\n",
        "    if state[\"error_log\"] and state[\"iterations\"] < 3:\n",
        "        print(\"!! Compiling failed. Agent is self-correcting...\")\n",
        "        return \"draft\"\n",
        "    return END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"extract\", extract_node)\n",
        "workflow.add_node(\"draft\", draft_slides_node)\n",
        "workflow.add_node(\"compile\", compile_node)\n",
        "\n",
        "workflow.set_entry_point(\"extract\")\n",
        "workflow.add_edge(\"extract\", \"draft\")\n",
        "workflow.add_edge(\"draft\", \"compile\")\n",
        "workflow.add_conditional_edges(\"compile\", route_after_compile)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 4. The Agent Runner with Feedback ---\n",
        "\n",
        "def run_agent(file_path):\n",
        "    # Initial run\n",
        "    current_state = {\"tar_path\": file_path, \"feedback\": None, \"error_log\": None, \"iterations\": 0}\n",
        "\n",
        "    while True:\n",
        "        final_state = app.invoke(current_state)\n",
        "\n",
        "        pdf_path = final_state.get('pdf_path')\n",
        "        if pdf_path and os.path.exists(pdf_path):\n",
        "            print(f\"\\n‚úÖ PDF Generated successfully at: {pdf_path}\")\n",
        "            print(\"You can download it manually from the file explorer.\")\n",
        "        else:\n",
        "            print(\"‚ùå Failed to compile after retries.\")\n",
        "            print(\"Current LaTeX Error:\", final_state.get('error_log'))\n",
        "\n",
        "        # HUMAN-IN-THE-LOOP SECTION\n",
        "        choice = input(\"\\nAre you happy with this PPT? (yes/no): \").strip().lower()\n",
        "        if choice == 'yes':\n",
        "            print(\"Agent stopped. Success!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"\\nWhat should I improve?\")\n",
        "            print(\"Options: [style, content, images,g maths, more slides]\")\n",
        "            user_critique = input(\"Your choice: \")\n",
        "\n",
        "            # Update state and loop back\n",
        "            current_state = final_state\n",
        "            current_state[\"feedback\"] = user_critique\n",
        "            current_state[\"iterations\"] = 0 # Reset iterations for the new attempt\n",
        "            print(\"\\nüîÑ Agent is refining based on your feedback...\")\n",
        "\n",
        "run_agent('/content/arXiv-2602.04039v1.tar.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6A9SfY_DZ0v"
      },
      "source": [
        "**Multi-Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egL-AZPVsSc_",
        "outputId": "2d2c938c-faaa-40b2-93ae-dc34d732d9e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Multi-Agent PPT Generator for arXiv-2602.04039v1.tar.gz...\n",
            "‚úÖ Extracted. Found 6 images.\n",
            "üß† Planner Agent is structuring the presentation...\n",
            "üíª Developer Agent is writing LaTeX code...\n",
            "‚öôÔ∏è Compiling PDF...\n",
            "\n",
            "‚úÖ SUCCESS! PDF generated at: /content/arXiv-2602.04039v1_work/presentation.pdf\n",
            "\n",
            "Are you happy with this PPT? (yes/no): no\n",
            "Enter your feedback (e.g., 'Make it more detailed', 'Fix slide 3'): make it more attractive, its very simple currently\n",
            "\n",
            "üîÑ Restarting Agents with your feedback...\n",
            "‚úÖ Extracted. Found 6 images.\n",
            "üß† Planner Agent is structuring the presentation...\n",
            "üíª Developer Agent is writing LaTeX code...\n",
            "‚öôÔ∏è Compiling PDF...\n",
            "\n",
            "‚úÖ SUCCESS! PDF generated at: /content/arXiv-2602.04039v1_work/presentation.pdf\n",
            "\n",
            "Are you happy with this PPT? (yes/no): yes\n",
            "üéâ Process Complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import subprocess\n",
        "import shutil\n",
        "from typing import TypedDict, List, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "MODEL_NAME = \"google/gemini-2.0-flash-001\"\n",
        "\n",
        "# --- 1. The Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    tar_path: str\n",
        "    work_dir: str\n",
        "    tex_content: str\n",
        "    presentation_plan: str  \n",
        "    beamer_code: str\n",
        "    pdf_path: str\n",
        "    available_images: List[str]\n",
        "    feedback: Optional[str]\n",
        "    error_log: Optional[str]\n",
        "    iterations: int\n",
        "\n",
        "# --- 2. Initializing the LLM (OpenRouter) ---\n",
        "def get_llm():\n",
        "    return ChatOpenAI(\n",
        "        api_key=\"\",\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        model=MODEL_NAME\n",
        "    )\n",
        "\n",
        "# --- 3. The Nodes ---\n",
        "\n",
        "def extract_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Extracts the tar file and finds the main .tex source and images.\n",
        "    \"\"\"\n",
        "    path = state['tar_path']\n",
        "    extract_to = os.path.abspath(path.replace(\".tar.gz\", \"_work\").replace(\".tar\", \"_work\"))\n",
        "\n",
        "    if os.path.exists(extract_to):\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(path, \"r:*\") as tar:\n",
        "            tar.extractall(path=extract_to, filter='fully_trusted')\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction warning: {e}\")\n",
        "\n",
        "    main_file = None\n",
        "    image_files = []\n",
        "\n",
        "    for root, _, fs in os.walk(extract_to):\n",
        "        for f in fs:\n",
        "            full_path = os.path.join(root, f)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.pdf')):\n",
        "                # Store relative path for LaTeX\n",
        "                rel_path = os.path.relpath(full_path, extract_to)\n",
        "                image_files.append(rel_path)\n",
        "\n",
        "            if f.endswith(\".tex\"):\n",
        "                with open(full_path, 'r', errors='ignore') as tex:\n",
        "                    if \"\\\\begin{document}\" in tex.read():\n",
        "                        main_file = full_path\n",
        "\n",
        "    if not main_file:\n",
        "        tex_files = [os.path.join(root, f) for root, _, fs in os.walk(extract_to) for f in fs if f.endswith('.tex')]\n",
        "        if tex_files:\n",
        "            main_file = max(tex_files, key=os.path.getsize)\n",
        "        else:\n",
        "            raise ValueError(\"Could not find a .tex file in the archive.\")\n",
        "\n",
        "    with open(main_file, 'r', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    print(f\"‚úÖ Extracted. Found {len(image_files)} images.\")\n",
        "\n",
        "    return {\n",
        "        \"work_dir\": extract_to,\n",
        "        \"tex_content\": content,\n",
        "        \"available_images\": image_files,\n",
        "        \"iterations\": 0\n",
        "    }\n",
        "\n",
        "def planner_agent_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    AGENT 1: The Planner.\n",
        "    Analyzes the paper and produces a structured outline (Plan).\n",
        "    Does NOT write LaTeX code.\n",
        "    \"\"\"\n",
        "    print(\"üß† Planner Agent is structuring the presentation...\")\n",
        "    llm = get_llm()\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Senior Research Communicator. Your goal is to structure a 10-12 slide presentation \"\n",
        "        \"based on the provided raw LaTeX paper content.\\n\"\n",
        "        \"Output a structured textual plan with:\\n\"\n",
        "        \"1. Slide Title\\n\"\n",
        "        \"2. Key Bullet Points (content to include)\\n\"\n",
        "        \"3. Suggested Visuals (if any valid image filenames are listed below).\\n\"\n",
        "        \"Do NOT write LaTeX code yet. Just the logical flow.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"Available Images: {state['available_images']}\\n\\nRaw Paper Content (Truncated): {state['tex_content'][:15000]}\"\n",
        "\n",
        "    # If there is feedback, the planner re-evaluates the plan\n",
        "    if state.get('feedback'):\n",
        "        user_prompt = f\"FEEDBACK FROM USER: {state['feedback']}\\n\\n\" + user_prompt\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "\n",
        "    return {\"presentation_plan\": response.content}\n",
        "\n",
        "def developer_agent_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    AGENT 2: The Developer.\n",
        "    Takes the Plan and writes the valid LaTeX Beamer code.\n",
        "    \"\"\"\n",
        "    print(\"üíª Developer Agent is writing LaTeX code...\")\n",
        "    llm = get_llm()\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a LaTeX Beamer Expert. Convert the provided 'Presentation Plan' into a high-quality, \"\n",
        "        \"compilable LaTeX Beamer presentation.\\n\"\n",
        "        \"RULES:\\n\"\n",
        "        \"1. Use \\\\documentclass{beamer}.\\n\"\n",
        "        \"2. Use \\\\usetheme{Madrid} or similar clean theme.\\n\"\n",
        "        \"3. ONLY use standard packages: graphicx, amsmath, amssymb, hyperref.\\n\"\n",
        "        \"4. Do NOT use custom .sty files or bibliography files (.bib).\\n\"\n",
        "        \"5. If images are used, ensure the filename matches exactly what is provided in the list.\\n\"\n",
        "        \"6. Wrap the code in ```latex ... ``` blocks.\\n\"\n",
        "        \"7. Ensure the code is complete (ends with \\\\end{document}).\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Here is the Presentation Plan:\\n{state['presentation_plan']}\\n\\n\"\n",
        "        f\"List of valid image files you can include using \\\\includegraphics: {state['available_images']}\\n\"\n",
        "    )\n",
        "\n",
        "    if state.get('error_log'):\n",
        "        user_prompt += f\"\\n\\n‚ö†Ô∏è PREVIOUS COMPILATION ERROR: {state['error_log']}\\nFix the code based on this error.\"\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "\n",
        "    # Clean formatting to get raw code\n",
        "    content = response.content\n",
        "    if \"```latex\" in content:\n",
        "        code = content.split(\"```latex\")[1].split(\"```\")[0].strip()\n",
        "    elif \"```\" in content:\n",
        "        code = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "    else:\n",
        "        code = content.strip()\n",
        "\n",
        "    return {\"beamer_code\": code, \"iterations\": state['iterations'] + 1}\n",
        "\n",
        "def compile_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Compiles the LaTeX code using pdflatex.\n",
        "    \"\"\"\n",
        "    print(\"‚öôÔ∏è Compiling PDF...\")\n",
        "    tex_file = os.path.join(state['work_dir'], \"presentation.tex\")\n",
        "\n",
        "    # Write the code\n",
        "    with open(tex_file, \"w\") as f:\n",
        "        f.write(state['beamer_code'])\n",
        "\n",
        "    # Compile (run twice for TOC/Labels if needed, but once is usually enough for draft)\n",
        "    result = subprocess.run(\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"presentation.tex\"],\n",
        "        cwd=state['work_dir'],\n",
        "        capture_output=True\n",
        "    )\n",
        "\n",
        "    stdout_log = result.stdout.decode('utf-8', errors='replace')\n",
        "    pdf_out = os.path.join(state['work_dir'], \"presentation.pdf\")\n",
        "\n",
        "    # Check logic\n",
        "    if os.path.exists(pdf_out) and result.returncode == 0:\n",
        "        return {\"pdf_path\": pdf_out, \"error_log\": None}\n",
        "    elif os.path.exists(pdf_out):\n",
        "        # PDF exists but might have minor errors\n",
        "        return {\"pdf_path\": pdf_out, \"error_log\": stdout_log[-1000:]}\n",
        "    else:\n",
        "        # Critical failure\n",
        "        return {\"error_log\": stdout_log[-1000:], \"pdf_path\": \"\"}\n",
        "\n",
        "# --- 4. Build Graph Logic ---\n",
        "\n",
        "def route_after_compile(state: AgentState):\n",
        "    if state[\"pdf_path\"] == \"\" and state[\"iterations\"] < 3:\n",
        "        print(f\"‚ùå Compilation failed. Retrying (Attempt {state['iterations']}/3)...\")\n",
        "        # If compilation fails, go back to Developer to fix syntax\n",
        "        return \"developer\"\n",
        "    return END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"extract\", extract_node)\n",
        "workflow.add_node(\"planner\", planner_agent_node)\n",
        "workflow.add_node(\"developer\", developer_agent_node)\n",
        "workflow.add_node(\"compile\", compile_node)\n",
        "\n",
        "# Add Edges\n",
        "workflow.set_entry_point(\"extract\")\n",
        "workflow.add_edge(\"extract\", \"planner\")\n",
        "workflow.add_edge(\"planner\", \"developer\")\n",
        "workflow.add_edge(\"developer\", \"compile\")\n",
        "\n",
        "# Conditional Edge: If compile fails, loop back to developer. Else finish.\n",
        "workflow.add_conditional_edges(\"compile\", route_after_compile, {\"developer\": \"developer\", END: END})\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 5. The Runner ---\n",
        "\n",
        "def run_multi_agent_system(file_path):\n",
        "    print(f\"üöÄ Starting Multi-Agent PPT Generator for {os.path.basename(file_path)}...\")\n",
        "\n",
        "    current_state = {\n",
        "        \"tar_path\": file_path,\n",
        "        \"feedback\": None,\n",
        "        \"error_log\": None,\n",
        "        \"iterations\": 0\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        # Run the graph\n",
        "        final_state = app.invoke(current_state)\n",
        "\n",
        "        pdf_path = final_state.get('pdf_path')\n",
        "        if pdf_path and os.path.exists(pdf_path):\n",
        "            print(f\"\\n‚úÖ SUCCESS! PDF generated at: {pdf_path}\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå FAILED to generate a valid PDF after retries.\")\n",
        "            if final_state.get('error_log'):\n",
        "                print(\"Last LaTeX Error snippet:\\n\", final_state['error_log'])\n",
        "\n",
        "        # Human-in-the-Loop Feedback\n",
        "        choice = input(\"\\nAre you happy with this PPT? (yes/no): \").strip().lower()\n",
        "        if choice == 'yes':\n",
        "            print(\"üéâ Process Complete.\")\n",
        "            break\n",
        "        else:\n",
        "            user_critique = input(\"Enter your feedback (e.g., 'Make it more detailed', 'Fix slide 3'): \")\n",
        "            print(\"\\nüîÑ Restarting Agents with your feedback...\")\n",
        "\n",
        "            # Reset state for refinement\n",
        "            # We go back to the Planner if content changes are needed\n",
        "            current_state = final_state\n",
        "            current_state[\"feedback\"] = user_critique\n",
        "            current_state[\"iterations\"] = 0\n",
        "            current_state[\"error_log\"] = None # Clear old errors so we don't confuse the model\n",
        "\n",
        "# Upload a tar.gz file to Colab first!\n",
        "run_multi_agent_system('/content/arXiv-2602.04039v1.tar.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBUPxrF3-v9v"
      },
      "source": [
        "**With frontend**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x0mF9o4RtIGU",
        "outputId": "83c3e866-88f9-4384-888c-f6a109e7d02d"
      },
      "outputs": [],
      "source": [
        "!pip install gradio langgraph langchain-openai\n",
        "# For pdflatex (if not installed, e.g., in Colab):\n",
        "!apt update && apt install -y texlive texlive-latex-extra texlive-fonts-recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "_RIDpDYGAz56",
        "outputId": "c74bb25e-c47f-46ff-f146-aed13c74b61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://577b5ce9edb2bf5ee3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://577b5ce9edb2bf5ee3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import subprocess\n",
        "import shutil\n",
        "from typing import TypedDict, List, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import gradio as gr\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_NAME = \"google/gemini-2.0-flash-001\"\n",
        "\n",
        "# --- 1. Define the Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    tar_path: str\n",
        "    work_dir: str\n",
        "    tex_content: str\n",
        "    presentation_plan: str\n",
        "    beamer_code: str\n",
        "    pdf_path: str\n",
        "    available_images: List[str]\n",
        "    feedback: Optional[str]\n",
        "    error_log: Optional[str]\n",
        "    iterations: int\n",
        "\n",
        "# --- 2. Initialize the LLM ---\n",
        "def get_llm():\n",
        "    return ChatOpenAI(\n",
        "        api_key=\"\",\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        model=MODEL_NAME\n",
        "    )\n",
        "\n",
        "# --- 3. The Nodes ---\n",
        "\n",
        "def extract_node(state: AgentState):\n",
        "    path = state['tar_path']\n",
        "    extract_to = os.path.abspath(path.replace(\".tar.gz\", \"_work\").replace(\".tar\", \"_work\"))\n",
        "\n",
        "    if os.path.exists(extract_to):\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(path, \"r:*\") as tar:\n",
        "            tar.extractall(path=extract_to, filter='fully_trusted')\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction warning: {e}\")\n",
        "\n",
        "    main_file = None\n",
        "    image_files = []\n",
        "\n",
        "    for root, _, fs in os.walk(extract_to):\n",
        "        for f in fs:\n",
        "            full_path = os.path.join(root, f)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.pdf')):\n",
        "                rel_path = os.path.relpath(full_path, extract_to)\n",
        "                image_files.append(rel_path)\n",
        "\n",
        "            if f.endswith(\".tex\"):\n",
        "                with open(full_path, 'r', errors='ignore') as tex:\n",
        "                    if \"\\\\begin{document}\" in tex.read():\n",
        "                        main_file = full_path\n",
        "\n",
        "    if not main_file:\n",
        "        tex_files = [os.path.join(root, f) for root, _, fs in os.walk(extract_to) for f in fs if f.endswith('.tex')]\n",
        "        if tex_files:\n",
        "            main_file = max(tex_files, key=os.path.getsize)\n",
        "        else:\n",
        "            raise ValueError(\"Could not find a .tex file in the archive.\")\n",
        "\n",
        "    with open(main_file, 'r', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    print(f\"‚úÖ Extracted. Found {len(image_files)} images.\")\n",
        "\n",
        "    return {\n",
        "        \"work_dir\": extract_to,\n",
        "        \"tex_content\": content,\n",
        "        \"available_images\": image_files,\n",
        "        \"iterations\": 0\n",
        "    }\n",
        "\n",
        "def planner_agent_node(state: AgentState):\n",
        "    print(\"üß† Planner Agent is structuring the presentation...\")\n",
        "    llm = get_llm()\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Senior Research Communicator. Your goal is to structure a 10-12 slide presentation \"\n",
        "        \"based on the provided raw LaTeX paper content.\\n\"\n",
        "        \"Output a structured textual plan with:\\n\"\n",
        "        \"1. Slide Title\\n\"\n",
        "        \"2. Key Bullet Points (content to include)\\n\"\n",
        "        \"3. Suggested Visuals (if any valid image filenames are listed below).\\n\"\n",
        "        \"Do NOT write LaTeX code yet. Just the logical flow.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"Available Images: {state['available_images']}\\n\\nRaw Paper Content (Truncated): {state['tex_content'][:15000]}\"\n",
        "\n",
        "    if state.get('feedback'):\n",
        "        user_prompt = f\"FEEDBACK FROM USER: {state['feedback']}\\n\\n\" + user_prompt\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "\n",
        "    return {\"presentation_plan\": response.content}\n",
        "\n",
        "def developer_agent_node(state: AgentState):\n",
        "    print(\"üíª Developer Agent is writing LaTeX code...\")\n",
        "    llm = get_llm()\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a LaTeX Beamer Expert. Convert the provided 'Presentation Plan' into a high-quality, \"\n",
        "        \"compilable LaTeX Beamer presentation.\\n\"\n",
        "        \"RULES:\\n\"\n",
        "        \"1. Use \\\\documentclass{beamer}.\\n\"\n",
        "        \"2. Use \\\\usetheme{Madrid} or similar clean theme.\\n\"\n",
        "        \"3. ONLY use standard packages: graphicx, amsmath, amssymb, hyperref.\\n\"\n",
        "        \"4. Do NOT use custom .sty files or bibliography files (.bib).\\n\"\n",
        "        \"5. If images are used, ensure the filename matches exactly what is provided in the list.\\n\"\n",
        "        \"6. Wrap the code in ```latex ... ``` blocks.\\n\"\n",
        "        \"7. Ensure the code is complete (ends with \\\\end{document}).\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Here is the Presentation Plan:\\n{state['presentation_plan']}\\n\\n\"\n",
        "        f\"List of valid image files you can include using \\\\includegraphics: {state['available_images']}\\n\"\n",
        "    )\n",
        "\n",
        "    if state.get('error_log'):\n",
        "        user_prompt += f\"\\n\\n‚ö†Ô∏è PREVIOUS COMPILATION ERROR: {state['error_log']}\\nFix the code based on this error.\"\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "\n",
        "    content = response.content\n",
        "    if \"```latex\" in content:\n",
        "        code = content.split(\"```latex\")[1].split(\"```\")[0].strip()\n",
        "    elif \"```\" in content:\n",
        "        code = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "    else:\n",
        "        code = content.strip()\n",
        "\n",
        "    return {\"beamer_code\": code, \"iterations\": state['iterations'] + 1}\n",
        "\n",
        "def compile_node(state: AgentState):\n",
        "    print(\"‚öôÔ∏è Compiling PDF...\")\n",
        "    tex_file = os.path.join(state['work_dir'], \"presentation.tex\")\n",
        "\n",
        "    with open(tex_file, \"w\") as f:\n",
        "        f.write(state['beamer_code'])\n",
        "\n",
        "    result = subprocess.run(\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"presentation.tex\"],\n",
        "        cwd=state['work_dir'],\n",
        "        capture_output=True\n",
        "    )\n",
        "\n",
        "    stdout_log = result.stdout.decode('utf-8', errors='replace')\n",
        "    pdf_out = os.path.join(state['work_dir'], \"presentation.pdf\")\n",
        "\n",
        "    if os.path.exists(pdf_out) and result.returncode == 0:\n",
        "        return {\"pdf_path\": pdf_out, \"error_log\": None}\n",
        "    elif os.path.exists(pdf_out):\n",
        "        return {\"pdf_path\": pdf_out, \"error_log\": stdout_log[-1000:]}\n",
        "    else:\n",
        "        return {\"error_log\": stdout_log[-1000:], \"pdf_path\": \"\"}\n",
        "\n",
        "# --- 4. Build Graph Logic ---\n",
        "\n",
        "def route_after_compile(state: AgentState):\n",
        "    if state[\"pdf_path\"] == \"\" and state[\"iterations\"] < 3:\n",
        "        print(f\"‚ùå Compilation failed. Retrying (Attempt {state['iterations']}/3)...\")\n",
        "        return \"developer\"\n",
        "    return END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"extract\", extract_node)\n",
        "workflow.add_node(\"planner\", planner_agent_node)\n",
        "workflow.add_node(\"developer\", developer_agent_node)\n",
        "workflow.add_node(\"compile\", compile_node)\n",
        "workflow.set_entry_point(\"extract\")\n",
        "workflow.add_edge(\"extract\", \"planner\")\n",
        "workflow.add_edge(\"planner\", \"developer\")\n",
        "workflow.add_edge(\"developer\", \"compile\")\n",
        "workflow.add_conditional_edges(\"compile\", route_after_compile, {\"developer\": \"developer\", END: END})\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- Gradio Frontend ---\n",
        "\n",
        "def process_tar(uploaded_file, feedback_text, current_state):\n",
        "    if uploaded_file is None and current_state is None:\n",
        "        return None, \"Please upload a compressed archive file.\", None\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        file_path = uploaded_file.name  # Gradio uploads to temp file; use .name for path\n",
        "        # Validate if it's a tar file (compressed or not)\n",
        "        if not tarfile.is_tarfile(file_path):\n",
        "            return None, \"Invalid file: Not a valid tar archive (supports .tar, .tar.gz, etc.).\", None\n",
        "        current_state = {\n",
        "            \"tar_path\": file_path,\n",
        "            \"feedback\": feedback_text if feedback_text else None,\n",
        "            \"error_log\": None,\n",
        "            \"iterations\": 0\n",
        "        }\n",
        "    else:\n",
        "        # Feedback on existing state\n",
        "        current_state[\"feedback\"] = feedback_text if feedback_text else None\n",
        "        current_state[\"iterations\"] = 0\n",
        "        current_state[\"error_log\"] = None\n",
        "\n",
        "    try:\n",
        "        final_state = app.invoke(current_state)\n",
        "        pdf_path = final_state.get('pdf_path')\n",
        "        if pdf_path and os.path.exists(pdf_path):\n",
        "            status = f\"‚úÖ PDF generated! Download below.\\nPlan:\\n{final_state['presentation_plan'][:500]}...\\nCode snippet:\\n{final_state['beamer_code'][:500]}...\"\n",
        "            return pdf_path, status, final_state  # Return PDF path for download, status, and updated state\n",
        "        else:\n",
        "            error = final_state.get('error_log', 'Unknown error')\n",
        "            return None, f\"‚ùå Failed: {error}\", current_state\n",
        "    except Exception as e:\n",
        "        return None, f\"Error: {str(e)}\", current_state\n",
        "\n",
        "with gr.Blocks(title=\"LaTeX to Beamer Presentation Generator\") as demo:\n",
        "    gr.Markdown(\"# Upload Compressed Archive File to Generate Presentation PDF\")\n",
        "\n",
        "    state = gr.State(None)  # Persist agent state across interactions\n",
        "\n",
        "    uploaded_file = gr.File(label=\"Upload Compressed Archive File\")  # Removed file_types to allow any, validate in code\n",
        "    feedback = gr.Textbox(label=\"Feedback (e.g., 'Make slides more detailed')\", placeholder=\"Optional for refinements\")\n",
        "    process_btn = gr.Button(\"Generate PDF\")\n",
        "\n",
        "    pdf_output = gr.File(label=\"Download Generated PDF\")\n",
        "    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    process_btn.click(\n",
        "        process_tar,\n",
        "        inputs=[uploaded_file, feedback, state],\n",
        "        outputs=[pdf_output, status, state]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)  # share=True for public link in Colab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
